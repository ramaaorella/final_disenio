<a name="top"></a>

<p align="right">
  <a href="https://github.com/ramaaorella/final_disenio/blob/main/add-process/design-iterations/iteration-2.md"><i><< Volver a la Iteración 2</i></a>
  &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="https://github.com/ramaaorella/final_disenio#proceso-add-e-iteraciones"> Volver al índice</a> 
</p>

## ADR 009: Paralelización de tareas en los servicios

Con el objetivo de mejorar el throughput del sistema y aumentar su capacidad de procesamiento ante un elevado volumen de mensajes entrantes y salientes, se considera la paralelización de tareas como una estrategia clave. Esta optimización se busca tanto a nivel de ejecución interna, mediante la utilización de hilos y procesos para ejecutar concurrentemente tareas dentro de un mismo servicio, como a nivel de replicación de servicios, distribuyendo la carga de trabajo entre múltiples instancias del mismo servicio.

### Decision

Se decide implementar la paralelización física de tareas en los servicios del sistema, utilizando hilos para mejorar la concurrencia dentro de cada servicio y dividiendo los flujos de mensajes (entrantes y salientes) en procesos dedicados. Se opta por la replicación de servicios para manejar la variabilidad en la carga de trabajo.

La cantidad de procesos asignados a cada flujo se configurará como una variable de entorno, lo que permitirá ajustar dinámicamente el throughput de cada flujo en particular durante el despliegue o reinicio de los servicios.

### Rationale

La implementación de la paralelización de tareas a nivel interno de los servicios permitirá aprovechar de manera más eficiente los recursos disponibles, reduciendo los tiempos de respuesta y aumentando el throughput. Esto optimiza el procesamiento al permitir que las operaciones se ejecuten en paralelo sin bloquear otras tareas, mejorando así el rendimiento del sistema y la capacidad de respuesta frente a cargas de trabajo altas.

La separación de los flujos de mensajes en procesos dedicados permite una gestión más controlada y aislada de los mensajes entrantes y salientes, lo que minimiza los cuellos de botella y mejora la resiliencia del sistema. Esta división también optimiza el manejo de cargas elevadas y previene problemas de starvation.

Por otro lado, la paralelización a nivel de replicación de servicios facilita la escalabilidad horizontal del sistema al distribuir la carga de trabajo entre múltiples instancias del mismo servicio. Al aumentar el número de instancias disponibles para procesar solicitudes, se mejora la capacidad global de procesamiento del sistema y se garantiza una mejor escalabilidad para hacer frente a aumentos repentinos en la demanda. Junto con la capacidad de ajustar dinámicamente el número de procesos asignados a cada flujo mediante variables de entorno se proporciona una mejor flexibilidad operativa, permitiendo que los parámetros de rendimiento se adapten rápidamente según las necesidades del sistema sin necesidad de realizar cambios en el código.

### Alternatives considered

Se consideró la opción de no implementar paralelización interna en los servicios y optar por un enfoque secuencial, donde los flujos de mensajes se procesan uno a la vez, gestionados a través de colas de mensajes y microservicios independientes. Aunque este enfoque podría mejorar la escalabilidad horizontal al distribuir el trabajo entre servicios, no optimizaría el rendimiento interno de cada servicio, lo que podría generar cuellos de botella en el procesamiento de tareas y afectar la capacidad de respuesta del sistema, especialmente durante picos de carga.

Otra alternativa fue paralelizar las tareas solo a nivel interno de los servicios, sin replicar los servicios. Aunque este enfoque podría mejorar el rendimiento dentro de cada servicio, se descartó porque la replicación de servicios es crucial para maximizar la eficiencia y la escalabilidad global del sistema.

También se evaluó la opción de no permitir la configuración dinámica de la cantidad de procesos dedicados a cada flujo a través de variables de entorno. Sin esta capacidad de ajuste, los parámetros de rendimiento estarían fijos, lo que dificultaría la adaptación a cambios en la carga del sistema durante el despliegue o reinicio de los servicios. Esta alternativa se descartó para mantener la flexibilidad operativa, especialmente en escenarios críticos.

Por último, se consideró separar los flujos de mensajes en servicios completamente independientes para los flujos entrantes y salientes. Si bien esta opción permitiría un aislamiento total de los flujos, lo que podría reducir riesgos de colapsos, se desestimó por la complejidad adicional que implicaría gestionar y orquestar un mayor número de servicios. La separación total de los flujos podría generar dificultades en el seguimiento de los mensajes y en la administración de los procesos, lo que no aportaría suficientes beneficios frente a la solución de paralelización interna combinada con la división de flujos en procesos dedicados.

### Status

Accepted

### Consequences

- Se mejora el rendimiento del sistema al procesar tareas de manera concurrente, reduciendo los tiempos de espera durante picos de carga.
- Se facilita la escalabilidad horizontal al permitir la replicación de servicios y la distribución de la carga de trabajo entre varias instancias.
- Se aumenta la flexibilidad en la configuración del número de procesos dedicados a cada flujo, permitiendo ajustes dinámicos durante el despliegue.
- Se reducen los cuellos de botella al dividir los flujos de mensajes en procesos dedicados para los flujos entrantes y salientes.
- Se mejora la resiliencia del sistema al aislar los flujos y reducir el riesgo de fallos globales.
- Se incrementa la complejidad operativa, ya que la gestión y orquestación de múltiples procesos y servicios requiere mayor coordinación.
- Depende de una infraestructura con múltiples núcleos de CPU para aprovechar plenamente los beneficios de la paralelización.

<p align="right">(<a href="#top">Volver al inicio</a>)</p>
